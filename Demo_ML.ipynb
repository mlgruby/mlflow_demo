{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bucket sagemaker-eu-west-1-505529183986\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import tarfile\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "sm_boto3 = boto3.client('sagemaker')\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "region = sess.boto_session.region_name\n",
    "\n",
    "bucket = sess.default_bucket()  # this could also be a hard-coded bucket name\n",
    "\n",
    "print('Using bucket ' + bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data from redshift and then prepare test and train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine('postgresql://mlflow_admin:EdBx6FUjDEgvG5@mlflow-redshift0.cdwamzbulp7n.eu-west-1.redshift.amazonaws.com:5439/dev')\n",
    "data_frame = pd.read_sql_query('SELECT * FROM boston_data;', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX = train_test_split(data_frame, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.to_csv('boston_train.csv')\n",
    "testX.to_csv('boston_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send data to S3. SageMaker will take training data from s3\n",
    "trainpath = sess.upload_data(\n",
    "    path='boston_train.csv', bucket=bucket,\n",
    "    key_prefix='sagemaker/sklearncontainer')\n",
    "\n",
    "testpath = sess.upload_data(\n",
    "    path='boston_test.csv', bucket=bucket,\n",
    "    key_prefix='sagemaker/sklearncontainer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a Script Mode script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import mlflow\n",
    "from mlflow.sklearn import log_model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "\n",
    "# inference functions ---------------\n",
    "def model_fn(model_dir):\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    print('extracting arguments')\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    # to simplify the demo we don't use all sklearn RandomForest hyperparameters\n",
    "    parser.add_argument('--n-estimators', type=int, default=10)\n",
    "    parser.add_argument('--min-samples-leaf', type=int, default=3)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--train-file', type=str, default='boston_train.csv')\n",
    "    parser.add_argument('--test-file', type=str, default='boston_test.csv')\n",
    "#     parser.add_argument('--data_table', type=str, default='boston_data')\n",
    "    parser.add_argument('--features', type=str)  # in this script we ask user to explicitly name features\n",
    "    parser.add_argument('--target', type=str) # in this script we ask user to explicitly name the target\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print('reading data')\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "#     engine = create_engine('postgresql://mlflow_admin:EdBx6FUjDEgvG5@mlflow-redshift0.cdwamzbulp7n.eu-west-1.redshift.amazonaws.com:5439/dev')\n",
    "#     data_frame = pd.read_sql_query('SELECT * FROM boston_data;', engine)\n",
    "#     train_df, test_df = train_test_split(data_frame, test_size=0.25, random_state=42)\n",
    "\n",
    "    print('building training and testing datasets')\n",
    "    X_train = train_df[args.features.split()]\n",
    "    X_test = test_df[args.features.split()]\n",
    "    y_train = train_df[args.target]\n",
    "    y_test = test_df[args.target]\n",
    "\n",
    "    # setting MLFlow tracker \n",
    "    uri = \"http://internal-a19641f33008a11eaa1590a387f0e3c9-331214759.eu-west-1.elb.amazonaws.com:5000\"\n",
    "    mlflow.set_tracking_uri(uri)\n",
    "    mlflow.set_experiment(\"ml-demo\")\n",
    "    \n",
    "    # train\n",
    "    print('training model')\n",
    "    with mlflow.start_run():\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=args.n_estimators,\n",
    "            min_samples_leaf=args.min_samples_leaf,\n",
    "            n_jobs=-1)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # print abs error\n",
    "        print('validating model')\n",
    "        abs_err = np.abs(model.predict(X_test) - y_test)\n",
    "\n",
    "        # print couple perf metrics\n",
    "        for q in [10, 50, 90]:\n",
    "            print('AE-at-' + str(q) + 'th-percentile: '\n",
    "                  + str(np.percentile(a=abs_err, q=q)))\n",
    "            mlflow.log_metric('E-at-' + str(q) + 'th-percentile', np.percentile(a=abs_err, q=q))\n",
    "\n",
    "        # persist model\n",
    "    #     path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    #     joblib.dump(model, path)\n",
    "    #     print('model persisted at ' + path)\n",
    "        print(args.min_samples_leaf)\n",
    "        \n",
    "        mlflow.log_param(\"n_estimators\", args.n_estimators)\n",
    "        mlflow.log_param(\"min_samples_leaf\", args.min_samples_leaf)\n",
    "        log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "extracting arguments\n",
      "reading data\n",
      "building training and testing datasets\n",
      "training model\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/IPython/lib/pretty.py:91: DeprecationWarning: IPython.utils.signatures backport for Python 2 is deprecated in IPython 6, which only supports Python 3\n",
      "  from IPython.utils.signatures import signature\n",
      "validating model\n",
      "AE-at-10th-percentile: 0.3830176667776669\n",
      "AE-at-50th-percentile: 1.761873781773776\n",
      "AE-at-90th-percentile: 5.094919928219925\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "! python script.py --n-estimators 150 \\\n",
    "                   --min-samples-leaf 4 \\\n",
    "                   --train /home/ec2-user/SageMaker/mlflow_demo \\\n",
    "                   --test /home/ec2-user/SageMaker/mlflow_demo \\\n",
    "                   --features 'crim zn indus chas nox rm age dis rad tax ptratio b lstat' \\\n",
    "                   --target target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching a training job with the Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point='script.py',\n",
    "    role = get_execution_role(),\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c5.xlarge',\n",
    "    framework_version='0.20.0',\n",
    "    metric_definitions=[\n",
    "        {'Name': 'median-AE',\n",
    "         'Regex': \"AE-at-50th-percentile: ([0-9.]+).*$\"}],\n",
    "    hyperparameters = {'n-estimators': 100,\n",
    "                       'min-samples-leaf': 3,\n",
    "                       'features': 'crim zn indus chas nox rm age dis rad tax ptratio b lstat',\n",
    "                       'target': 'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch training job, with asynchronous call\n",
    "sklearn_estimator.fit({'train':trainpath, 'test': testpath}, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.sklearn import log_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
